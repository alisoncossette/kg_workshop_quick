{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG Patterns\n",
    "\n",
    "This notebook demonstrates key GraphRAG patterns that combine:\n",
    "1. Graph-based context retrieval\n",
    "2. Vector embeddings for semantic search\n",
    "3. LLM integration for natural language understanding\n",
    "4. RAG-optimized prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Neo4jVector\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Neo4j connection\n",
    "driver = GraphDatabase.driver(\n",
    "    os.getenv('NEO4J_URI'),\n",
    "    auth=(os.getenv('NEO4J_USERNAME'), os.getenv('NEO4J_PASSWORD'))\n",
    ")\n",
    "\n",
    "# Initialize LLM and embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 1: Hybrid Graph-Vector Search\n",
    "\n",
    "This pattern combines graph traversal with semantic search to find relevant information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query, k=3):\n",
    "    # Create vector store\n",
    "    vector_store = Neo4jVector.from_existing_index(\n",
    "        embeddings,\n",
    "        driver=driver,\n",
    "        node_label='Document',\n",
    "        embedding_node_property='embedding',\n",
    "        text_node_property='content'\n",
    "    )\n",
    "    \n",
    "    # Step 1: Semantic search for relevant documents\n",
    "    similar_docs = vector_store.similarity_search(query, k=k)\n",
    "    \n",
    "    # Step 2: Graph traversal to find connected entities\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"\"\"\n",
    "        // Find documents similar to the query\n",
    "        WITH $doc_ids as doc_ids\n",
    "        MATCH (d:Document)\n",
    "        WHERE d.id IN doc_ids\n",
    "        \n",
    "        // Find connected entities within 2 hops\n",
    "        MATCH path = (d)-[*1..2]-(related)\n",
    "        WHERE NOT related:Document  // Exclude other documents\n",
    "        \n",
    "        // Return unique paths and relevance score\n",
    "        RETURN DISTINCT path,\n",
    "               d.similarity as relevance\n",
    "        ORDER BY relevance DESC\n",
    "        \"\"\", doc_ids=[doc.metadata['id'] for doc in similar_docs])\n",
    "        \n",
    "        return result.data()\n",
    "\n",
    "# Example usage\n",
    "results = hybrid_search(\"How do I troubleshoot laptop connectivity issues?\")\n",
    "print(f\"Found {len(results)} relevant paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 2: Context-Aware RAG\n",
    "\n",
    "This pattern enhances RAG with graph-based context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_aware_rag(query, context_type='product'):\n",
    "    # Step 1: Extract entities and relationships from query\n",
    "    extraction_prompt = f\"Extract key entities and their relationships from: {query}\"\n",
    "    entities = llm.predict(extraction_prompt)\n",
    "    \n",
    "    # Step 2: Graph traversal for context\n",
    "    with driver.session() as session:\n",
    "        context = session.run(\"\"\"\n",
    "        // Find relevant context based on entity type\n",
    "        MATCH (e:{context_type})\n",
    "        WHERE e.name CONTAINS $entity\n",
    "        \n",
    "        // Get connected documents with embeddings\n",
    "        MATCH (e)-[r]-(d:Document)\n",
    "        WHERE d.embedding IS NOT NULL\n",
    "        \n",
    "        // Return context and documents\n",
    "        RETURN e, collect(d) as docs\n",
    "        \"\"\", context_type=context_type, entity=entities)\n",
    "    \n",
    "    # Step 3: Semantic search within context\n",
    "    vector_store = Neo4jVector.from_existing_index(\n",
    "        embeddings,\n",
    "        driver=driver,\n",
    "        node_label='Document',\n",
    "        embedding_node_property='embedding',\n",
    "        text_node_property='content'\n",
    "    )\n",
    "    \n",
    "    relevant_docs = vector_store.similarity_search(\n",
    "        query,\n",
    "        k=3,\n",
    "        filter={\"context\": context_type}\n",
    "    )\n",
    "    \n",
    "    # Step 4: Generate response with context\n",
    "    response = llm.predict(\n",
    "        f\"Based on these documents: {relevant_docs}\\n\\n\"\n",
    "        f\"And this context: {context}\\n\\n\"\n",
    "        f\"Answer: {query}\"\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "answer = context_aware_rag(\n",
    "    \"What are common issues with the Laptop Pro model?\",\n",
    "    context_type='Product'\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 3: Memory-Augmented GraphRAG\n",
    "\n",
    "This pattern maintains conversation history in a graph structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_augmented_rag(query, session_id):\n",
    "    # Step 1: Store query in conversation graph\n",
    "    with driver.session() as session:\n",
    "        session.run(\"\"\"\n",
    "        MATCH (s:Session {id: $session_id})\n",
    "        CREATE (q:Query {text: $query, timestamp: datetime()})\n",
    "        CREATE (s)-[:HAS_QUERY]->(q)\n",
    "        \"\"\", session_id=session_id, query=query)\n",
    "    \n",
    "    # Step 2: Get conversation history and context\n",
    "    with driver.session() as session:\n",
    "        history = session.run(\"\"\"\n",
    "        MATCH (s:Session {id: $session_id})-[:HAS_QUERY]->(q)\n",
    "        WITH q ORDER BY q.timestamp\n",
    "        RETURN collect(q.text) as queries,\n",
    "               collect(q.response) as responses\n",
    "        \"\"\", session_id=session_id)\n",
    "    \n",
    "    # Step 3: Combine with vector search\n",
    "    vector_store = Neo4jVector.from_existing_index(\n",
    "        embeddings,\n",
    "        driver=driver,\n",
    "        node_label='Document',\n",
    "        embedding_node_property='embedding',\n",
    "        text_node_property='content'\n",
    "    )\n",
    "    \n",
    "    relevant_docs = vector_store.similarity_search(\n",
    "        query + \" \" + \" \".join(history['queries']),\n",
    "        k=3\n",
    "    )\n",
    "    \n",
    "    # Step 4: Generate response with history and context\n",
    "    response = llm.predict(\n",
    "        f\"Conversation history: {history}\\n\\n\"\n",
    "        f\"Relevant documents: {relevant_docs}\\n\\n\"\n",
    "        f\"Current query: {query}\\n\\n\"\n",
    "        f\"Generate a response that maintains conversation context.\"\n",
    "    )\n",
    "    \n",
    "    # Step 5: Store response in graph\n",
    "    with driver.session() as session:\n",
    "        session.run(\"\"\"\n",
    "        MATCH (s:Session {id: $session_id})-[:HAS_QUERY]->(q)\n",
    "        WHERE q.text = $query\n",
    "        SET q.response = $response\n",
    "        \"\"\", session_id=session_id, query=query, response=response)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "response = memory_augmented_rag(\n",
    "    \"Tell me more about that issue\",\n",
    "    session_id=\"session_123\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key GraphRAG Concepts Demonstrated\n",
    "\n",
    "1. **Hybrid Search**: Combining vector similarity with graph traversal\n",
    "2. **Context-Aware RAG**: Using graph relationships to enhance context\n",
    "3. **Memory Augmentation**: Storing conversation state in graph structure\n",
    "4. **Entity Extraction**: Using LLMs to identify graph entities\n",
    "5. **Semantic Search**: Vector embeddings for content similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

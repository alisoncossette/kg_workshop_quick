{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG Patterns\n",
    "\n",
    "In this notebook, we'll explore advanced GraphRAG patterns that combine vector search with graph traversal. We'll cover:\n",
    "\n",
    "1. Graph-enhanced retrieval\n",
    "2. Text2Cypher for natural language queries\n",
    "3. Making AI decisions explainable\n",
    "4. Building LLM chains with graph context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "from neo4j_graphrag.llm import OpenAILLM\n",
    "from neo4j_graphrag.embedder import OpenAIEmbedder\n",
    "from neo4j_graphrag.retriever import VectorRetriever, VectorCypherRetriever\n",
    "from neo4j_graphrag.text2cypher import Text2Cypher\n",
    "\n",
    "# Setup\n",
    "load_dotenv()\n",
    "driver = GraphDatabase.driver(\n",
    "    os.getenv('NEO4J_URI'),\n",
    "    auth=(os.getenv('NEO4J_USERNAME'), os.getenv('NEO4J_PASSWORD'))\n",
    ")\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "llm = OpenAILLM()\n",
    "embedder = OpenAIEmbedder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Graph-Enhanced Retrieval\n",
    "\n",
    "Let's see how combining vector search with graph traversal improves results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector-cypher retriever\n",
    "graph_retriever = VectorCypherRetriever(\n",
    "    driver=driver,\n",
    "    embedder=embedder,\n",
    "    node_label=\"Product\",\n",
    "    embedding_property=\"embedding\",\n",
    "    cypher_template=\"\"\"\n",
    "    MATCH (p:Product)\n",
    "    WHERE p.embedding IS NOT NULL\n",
    "    WITH p, gds.similarity.cosine(p.embedding, $query_embedding) AS score\n",
    "    WHERE score > 0.7\n",
    "    \n",
    "    // Enhance with graph context\n",
    "    OPTIONAL MATCH (p)-[:HAS_FEATURE]->(f:Feature)\n",
    "    OPTIONAL MATCH (p)-[:COMPATIBLE_WITH]->(c:Product)\n",
    "    \n",
    "    RETURN p.description as text,\n",
    "           score,\n",
    "           collect(DISTINCT f.name) as features,\n",
    "           collect(DISTINCT c.name) as compatible_products\n",
    "    ORDER BY score DESC\n",
    "    LIMIT 3\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Try the complex query from last notebook\n",
    "query = \"Find accessories compatible with the XPS 13 that improve productivity\"\n",
    "results = graph_retriever.retrieve(query)\n",
    "\n",
    "print(\"Graph-enhanced results (with context):\")\n",
    "for r in results:\n",
    "    print(f\"\\nScore: {r.score:.2f}\")\n",
    "    print(f\"Product: {r.text}\")\n",
    "    print(f\"Features: {r.features}\")\n",
    "    print(f\"Compatible with: {r.compatible_products}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text2Cypher for Natural Language Queries\n",
    "\n",
    "Convert natural language to Cypher queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Text2Cypher\n",
    "text2cypher = Text2Cypher(\n",
    "    driver=driver,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Example queries\n",
    "queries = [\n",
    "    \"What products are compatible with the XPS 13?\",\n",
    "    \"Find gaming accessories with RGB features\",\n",
    "    \"Show me products that improve productivity and have good reviews\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    cypher = text2cypher.translate(query)\n",
    "    print(f\"Cypher: {cypher}\")\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        results = session.run(cypher)\n",
    "        for record in results:\n",
    "            print(f\"Result: {record}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Making AI Decisions Explainable\n",
    "\n",
    "Track how the LLM uses graph data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_recommendation(query, results):\n",
    "    explanation_prompt = f\"\"\"\n",
    "    Query: {query}\n",
    "    \n",
    "    Retrieved Information:\n",
    "    {results}\n",
    "    \n",
    "    Explain why these results are relevant, considering:\n",
    "    1. Vector similarity scores\n",
    "    2. Graph relationships\n",
    "    3. Product features\n",
    "    \"\"\"\n",
    "    \n",
    "    explanation = llm.complete(explanation_prompt)\n",
    "    return explanation\n",
    "\n",
    "# Example recommendation\n",
    "query = \"Need productivity tools for my XPS laptop\"\n",
    "results = graph_retriever.retrieve(query)\n",
    "explanation = explain_recommendation(query, results)\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building LLM Chains with Graph Context\n",
    "\n",
    "Create a complete recommendation chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_products(user_query):\n",
    "    # Step 1: Convert to Cypher\n",
    "    cypher = text2cypher.translate(user_query)\n",
    "    \n",
    "    # Step 2: Get graph context\n",
    "    with driver.session() as session:\n",
    "        graph_results = session.run(cypher)\n",
    "    \n",
    "    # Step 3: Vector similarity search\n",
    "    vector_results = graph_retriever.retrieve(user_query)\n",
    "    \n",
    "    # Step 4: Combine and explain\n",
    "    combined_results = {\n",
    "        'graph': graph_results,\n",
    "        'vector': vector_results\n",
    "    }\n",
    "    \n",
    "    final_prompt = f\"\"\"\n",
    "    User Query: {user_query}\n",
    "    \n",
    "    Graph Results: {combined_results['graph']}\n",
    "    Vector Results: {combined_results['vector']}\n",
    "    \n",
    "    Provide a detailed recommendation that:\n",
    "    1. Suggests specific products\n",
    "    2. Explains why they match the user's needs\n",
    "    3. Includes compatibility information\n",
    "    4. Mentions alternative options\n",
    "    \"\"\"\n",
    "    \n",
    "    recommendation = llm.complete(final_prompt)\n",
    "    return recommendation\n",
    "\n",
    "# Try the recommendation chain\n",
    "query = \"Looking for high-performance accessories for video editing on my XPS laptop\"\n",
    "recommendation = recommend_products(query)\n",
    "print(recommendation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In the next notebook, we'll explore how to build a memory graph to track conversation context and user preferences!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

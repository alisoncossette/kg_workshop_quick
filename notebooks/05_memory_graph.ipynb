{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Memory Graph\n",
    "\n",
    "In this notebook, we'll explore how to build a memory graph to enhance GraphRAG with conversational context. We'll cover:\n",
    "\n",
    "1. Memory graph structure\n",
    "2. Tracking conversations and decisions\n",
    "3. Using memory for better recommendations\n",
    "4. Building a complete conversational agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "from datetime import datetime\n",
    "from neo4j_graphrag.llm import OpenAILLM\n",
    "from neo4j_graphrag.embedder import OpenAIEmbedder\n",
    "from neo4j_graphrag.retriever import VectorRetriever, VectorCypherRetriever\n",
    "from neo4j_graphrag.text2cypher import Text2Cypher\n",
    "\n",
    "# Setup\n",
    "load_dotenv()\n",
    "driver = GraphDatabase.driver(\n",
    "    os.getenv('NEO4J_URI'),\n",
    "    auth=(os.getenv('NEO4J_USERNAME'), os.getenv('NEO4J_PASSWORD'))\n",
    ")\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "llm = OpenAILLM()\n",
    "embedder = OpenAIEmbedder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Memory Graph Structure\n",
    "\n",
    "Let's create a memory graph schema that tracks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_memory_graph():\n",
    "    with driver.session() as session:\n",
    "        # Create constraints\n",
    "        session.run(\"\"\"\n",
    "        CREATE CONSTRAINT memory_id IF NOT EXISTS\n",
    "        FOR (m:Memory) REQUIRE m.id IS UNIQUE\n",
    "        \"\"\")\n",
    "        \n",
    "        session.run(\"\"\"\n",
    "        CREATE CONSTRAINT conversation_id IF NOT EXISTS\n",
    "        FOR (c:Conversation) REQUIRE c.id IS UNIQUE\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create indexes\n",
    "        session.run(\"\"\"\n",
    "        CREATE INDEX memory_embedding IF NOT EXISTS\n",
    "        FOR (m:Memory)\n",
    "        ON (m.embedding)\n",
    "        \"\"\")\n",
    "\n",
    "setup_memory_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tracking Conversations and Decisions\n",
    "\n",
    "Create a system to record interactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationTracker:\n",
    "    def __init__(self, driver, llm, embedder):\n",
    "        self.driver = driver\n",
    "        self.llm = llm\n",
    "        self.embedder = embedder\n",
    "        \n",
    "    def start_conversation(self):\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "            CREATE (c:Conversation {\n",
    "                id: randomUUID(),\n",
    "                started: datetime(),\n",
    "                active: true\n",
    "            })\n",
    "            RETURN c.id as conversation_id\n",
    "            \"\"\")\n",
    "            return result.single()[\"conversation_id\"]\n",
    "    \n",
    "    def record_interaction(self, conversation_id, user_input, system_response, context_used=None):\n",
    "        with self.driver.session() as session:\n",
    "            # Create memory embedding\n",
    "            memory_text = f\"User: {user_input}\\nSystem: {system_response}\"\n",
    "            embedding = self.embedder.embed_text(memory_text)\n",
    "            \n",
    "            # Record in graph\n",
    "            session.run(\"\"\"\n",
    "            MATCH (c:Conversation {id: $conversation_id})\n",
    "            \n",
    "            CREATE (m:Memory {\n",
    "                id: randomUUID(),\n",
    "                timestamp: datetime(),\n",
    "                user_input: $user_input,\n",
    "                system_response: $system_response,\n",
    "                embedding: $embedding,\n",
    "                context_used: $context_used\n",
    "            })\n",
    "            \n",
    "            CREATE (c)-[:CONTAINS]->(m)\n",
    "            \n",
    "            WITH m\n",
    "            MATCH (prev:Memory)\n",
    "            WHERE prev.timestamp < m.timestamp\n",
    "            WITH m, prev\n",
    "            ORDER BY prev.timestamp DESC\n",
    "            LIMIT 1\n",
    "            CREATE (prev)-[:NEXT]->(m)\n",
    "            \"\"\", conversation_id=conversation_id,\n",
    "                 user_input=user_input,\n",
    "                 system_response=system_response,\n",
    "                 embedding=embedding,\n",
    "                 context_used=context_used)\n",
    "\n",
    "# Create tracker\n",
    "tracker = ConversationTracker(driver, llm, embedder)\n",
    "\n",
    "# Example usage\n",
    "conversation_id = tracker.start_conversation()\n",
    "tracker.record_interaction(\n",
    "    conversation_id,\n",
    "    \"What laptops do you recommend for video editing?\",\n",
    "    \"Based on your needs, I recommend the XPS 17 with its powerful GPU and 4K display.\",\n",
    "    {\"products_mentioned\": [\"XPS 17\"], \"features_considered\": [\"GPU\", \"4K display\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Memory for Better Recommendations\n",
    "\n",
    "Enhance recommendations with conversation history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryAwareRecommender:\n",
    "    def __init__(self, driver, llm, embedder):\n",
    "        self.driver = driver\n",
    "        self.llm = llm\n",
    "        self.embedder = embedder\n",
    "        self.retriever = VectorCypherRetriever(\n",
    "            driver=driver,\n",
    "            embedder=embedder,\n",
    "            node_label=\"Memory\",\n",
    "            embedding_property=\"embedding\"\n",
    "        )\n",
    "    \n",
    "    def get_conversation_context(self, conversation_id):\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "            MATCH (c:Conversation {id: $conversation_id})-[:CONTAINS]->(m:Memory)\n",
    "            RETURN m.user_input as input,\n",
    "                   m.system_response as response,\n",
    "                   m.context_used as context\n",
    "            ORDER BY m.timestamp DESC\n",
    "            LIMIT 5\n",
    "            \"\"\", conversation_id=conversation_id)\n",
    "            return list(result)\n",
    "    \n",
    "    def recommend(self, conversation_id, user_input):\n",
    "        # Get conversation history\n",
    "        history = self.get_conversation_context(conversation_id)\n",
    "        \n",
    "        # Find similar past interactions\n",
    "        similar_memories = self.retriever.retrieve(user_input)\n",
    "        \n",
    "        # Build context-aware prompt\n",
    "        prompt = f\"\"\"\n",
    "        User Query: {user_input}\n",
    "        \n",
    "        Conversation History:\n",
    "        {history}\n",
    "        \n",
    "        Similar Past Interactions:\n",
    "        {similar_memories}\n",
    "        \n",
    "        Provide a recommendation that:\n",
    "        1. Considers the user's current query\n",
    "        2. Takes into account their conversation history\n",
    "        3. Learns from similar past interactions\n",
    "        4. Maintains consistency with previous responses\n",
    "        \"\"\"\n",
    "        \n",
    "        recommendation = self.llm.complete(prompt)\n",
    "        return recommendation\n",
    "\n",
    "# Create recommender\n",
    "recommender = MemoryAwareRecommender(driver, llm, embedder)\n",
    "\n",
    "# Example usage\n",
    "response = recommender.recommend(\n",
    "    conversation_id,\n",
    "    \"What accessories would work well with that laptop?\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building a Complete Conversational Agent\n",
    "\n",
    "Put it all together into a GraphRAG agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphRAGAgent:\n",
    "    def __init__(self, driver, llm, embedder):\n",
    "        self.tracker = ConversationTracker(driver, llm, embedder)\n",
    "        self.recommender = MemoryAwareRecommender(driver, llm, embedder)\n",
    "        self.text2cypher = Text2Cypher(driver, llm)\n",
    "    \n",
    "    def chat(self, conversation_id, user_input):\n",
    "        # Get recommendation\n",
    "        response = self.recommender.recommend(conversation_id, user_input)\n",
    "        \n",
    "        # Record interaction\n",
    "        self.tracker.record_interaction(conversation_id, user_input, response)\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Create agent\n",
    "agent = GraphRAGAgent(driver, llm, embedder)\n",
    "\n",
    "# Example conversation\n",
    "conversation_id = agent.tracker.start_conversation()\n",
    "\n",
    "queries = [\n",
    "    \"I need a laptop for video editing and 3D rendering\",\n",
    "    \"What accessories would you recommend?\",\n",
    "    \"Tell me more about the display options\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nUser: {query}\")\n",
    "    response = agent.chat(conversation_id, query)\n",
    "    print(f\"Agent: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "You've now completed the GraphRAG workshop! Try:\n",
    "1. Customizing the memory graph structure\n",
    "2. Adding more sophisticated context tracking\n",
    "3. Implementing your own GraphRAG patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
